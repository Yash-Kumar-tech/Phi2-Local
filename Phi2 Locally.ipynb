{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing necessary modules\n",
    "1. snapshot_download from huggingface_hub allows to download either a single file or an entire repository stored on the Huggingface Hub. The downloaded files are cached on the local disk and downloads are concurrent for higher speeds.\n",
    "2. time module can be used to calculate the execution time and speed of the model. \n",
    "3. AutoModelForCausalLM is a class that can be used to load a pre-trained causal language model from the HuggingFace Hub. It can be used to generate text, answer questions and perform other NLP tasks.\n",
    "4. AutoTokenizer is a class that can be used to tokenize text for use with a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the official phi-2 repository on Hugging Face published by Microsoft. The entire repository is roughly 5.18GB in size and therefore the download may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"microsoft/phi-2\"\n",
    "repo_type = \"model\"\n",
    "local_dir = \"./phi-2\"\n",
    "local_dir_use_symlinks = False\n",
    "model_path = snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=repo_type,\n",
    "    local_dir=local_dir,\n",
    "    local_dir_use_symlinks=local_dir_use_symlinks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phi-2 has not been fine-tuned to provide more freedom to the research community. Although the model isn't fine-tuned, it still performs well for some general QA, text generation tasks. We now set-up the model, tokenizer and write a generate function to get the model to generate text based on prompts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype = \"auto\",\n",
    "    flash_attn = True,\n",
    "    flash_rotary = True,\n",
    "    fused_dense = True,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    trust_remote_code = True\n",
    ") \n",
    "\n",
    "def generate(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_length=1000)\n",
    "    decoded_output = tokenizer.batch_decode(outputs)\n",
    "    return decoded_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is now setup, we can now use it to generate text or answer questions by calling the generate function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text = generate('''def print_prime(n):\n",
    "    \"\"\"\n",
    "    Print all primes between 1 and n\n",
    "    \"\"\"'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
